{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:\n",
      " 3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 14:00:49) [MSC v.1915 64 bit (AMD64)]\n",
      "Path to the python executable:\n",
      " C:\\Users\\sneha\\.conda\\envs\\decathlon\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import platform\n",
    "print(\"Python version:\\n\", sys.version)\n",
    "print (\"Path to the python executable:\\n\", sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biomedical Image Segmentation with U-Net\n",
    "\n",
    "In this code example, we apply the U-Net architecture to segment brain tumors from raw MRI scans as shown below. With relatively little data we are able to train a U-Net model to accurately predict where tumors exist. \n",
    "\n",
    "The Dice coefficient (the standard metric for the BraTS dataset used in the study) for our model is about 0.82-0.88.  Menze et al. [reported](http://ieeexplore.ieee.org/document/6975210/) that expert neuroradiologists manually segmented these tumors with a cross-rater Dice score of 0.75-0.85, meaning that the model’s predictions are on par with what expert physicians have made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/figure1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since its introduction two years ago, the [U-Net](https://arxiv.org/pdf/1505.04597.pdf0) architecture has been used to create deep learning models for segmenting [nerves](https://github.com/jocicmarko/ultrasound-nerve-segmentation) in ultrasound images, [lungs](https://www.kaggle.com/c/data-science-bowl-2017#tutorial) in CT scans, and even [interference](https://github.com/jakeret/tf_unet) in radio telescopes.\n",
    "\n",
    "## What is U-Net?\n",
    "U-Net is designed like an [auto-encoder](https://en.wikipedia.org/wiki/Autoencoder). It has an encoding path (“contracting”) paired with a decoding path (“expanding”) which gives it the “U” shape.  However, in contrast to the autoencoder, U-Net predicts a pixelwise segmentation map of the input image rather than classifying the input image as a whole. For each pixel in the original image, it asks the question: “To which class does this pixel belong?” This flexibility allows U-Net to predict different parts of the tumor simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/unet.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module loads the data from `data.py`, creates a TensorFlow/Keras model from `model.py`, trains the model on the data, and then saves the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using Tensorflow version 1.11.0 with Intel(R) MKL enabled\n",
      "Keras API version: 2.2.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "from IPython.display import Image\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import settings    # Use the custom settings.py file for default parameters\n",
    "# import onnxmltools\n",
    "\n",
    "import sys; sys.argv=['']; del sys\n",
    "# from model import load_model, get_callbacks, evaluate_model\n",
    "from data import load_data\n",
    "from model import unet\n",
    "\n",
    "from argparser import args\n",
    "\n",
    "if args.keras_api:\n",
    "    import keras as K\n",
    "else:\n",
    "    from tensorflow import keras as K\n",
    "\n",
    "print (\"We are using Tensorflow version\", tf.__version__,\\\n",
    "       \"with Intel(R) MKL\", \"enabled\" if tf.pywrap_tensorflow.IsMklEnabled() else \"disabled\",)\n",
    "print(\"Keras API version: {}\".format(K.__version__))\n",
    "\n",
    "onnx = False # Set whether we are exporting to ONNX model and using nGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For best CPU speed set the number of intra and inter threads to take advantage of multi-core systems.\n",
    "See https://github.com/intel/mkl-dnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Set the multi-threading parameters for Tensorflow. \n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=args.num_threads,\n",
    "                        inter_op_parallelism_threads=args.num_inter_threads)\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "K.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bulk of the training section can be broken down in 4 simple steps:\n",
    "1. Load the training data\n",
    "1. Define the model\n",
    "3. Train the model on the data\n",
    "4. Evaluate the best model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 : Loading the datafrom the `HDF5` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full path to the HFS file: ../../data/decathlon/144x144/Task01_BrainTumour.h5\n"
     ]
    }
   ],
   "source": [
    "hdf5_filename = os.path.join(args.data_path, args.data_filename)\n",
    "print (\"Full path to the HFS file:\", hdf5_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size = 128\n",
      "Training image dimensions:   (58464, 144, 144, 4)\n",
      "Training mask dimensions:    (58464, 144, 144, 1)\n",
      "Validation image dimensions: (4608, 144, 144, 4)\n",
      "Validation mask dimensions:  (4608, 144, 144, 1)\n",
      "Testing image dimensions: (6624, 144, 144, 4)\n",
      "Testing mask dimensions:  (6624, 144, 144, 1)\n"
     ]
    }
   ],
   "source": [
    "imgs_train, msks_train, imgs_validation, msks_validation, imgs_testing, msks_testing = \\\n",
    "    load_data(hdf5_filename, args.batch_size,[args.crop_dim, args.crop_dim])\n",
    "\n",
    "np.random.seed(816)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Creating and compiling model ...\n",
      "------------------------------\n",
      "Data format = channels_last\n",
      "Using UpSampling2D\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "MRImages (InputLayer)           (None, 144, 144, 4)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encodeAa (Conv2D)               (None, 144, 144, 32) 1184        MRImages[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "encodeAb (Conv2D)               (None, 144, 144, 32) 9248        encodeAa[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "poolA (MaxPooling2D)            (None, 72, 72, 32)   0           encodeAb[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "encodeBa (Conv2D)               (None, 72, 72, 64)   18496       poolA[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "encodeBb (Conv2D)               (None, 72, 72, 64)   36928       encodeBa[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "poolB (MaxPooling2D)            (None, 36, 36, 64)   0           encodeBb[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "encodeCa (Conv2D)               (None, 36, 36, 128)  73856       poolB[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_1 (SpatialDro (None, 36, 36, 128)  0           encodeCa[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "encodeCb (Conv2D)               (None, 36, 36, 128)  147584      spatial_dropout2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "poolC (MaxPooling2D)            (None, 18, 18, 128)  0           encodeCb[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "encodeDa (Conv2D)               (None, 18, 18, 256)  295168      poolC[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_2 (SpatialDro (None, 18, 18, 256)  0           encodeDa[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "encodeDb (Conv2D)               (None, 18, 18, 256)  590080      spatial_dropout2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "poolD (MaxPooling2D)            (None, 9, 9, 256)    0           encodeDb[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "encodeEa (Conv2D)               (None, 9, 9, 512)    1180160     poolD[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "encodeEb (Conv2D)               (None, 9, 9, 512)    2359808     encodeEa[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "upE (UpSampling2D)              (None, 18, 18, 512)  0           encodeEb[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatD (Concatenate)           (None, 18, 18, 768)  0           upE[0][0]                        \n",
      "                                                                 encodeDb[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "decodeCa (Conv2D)               (None, 18, 18, 256)  1769728     concatD[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "decodeCb (Conv2D)               (None, 18, 18, 256)  590080      decodeCa[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "upC (UpSampling2D)              (None, 36, 36, 256)  0           decodeCb[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatC (Concatenate)           (None, 36, 36, 384)  0           upC[0][0]                        \n",
      "                                                                 encodeCb[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "decodeBa (Conv2D)               (None, 36, 36, 128)  442496      concatC[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "decodeBb (Conv2D)               (None, 36, 36, 128)  147584      decodeBa[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "upB (UpSampling2D)              (None, 72, 72, 128)  0           decodeBb[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatB (Concatenate)           (None, 72, 72, 192)  0           upB[0][0]                        \n",
      "                                                                 encodeBb[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "decodeAa (Conv2D)               (None, 72, 72, 64)   110656      concatB[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "decodeAb (Conv2D)               (None, 72, 72, 64)   36928       decodeAa[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "upA (UpSampling2D)              (None, 144, 144, 64) 0           decodeAb[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatA (Concatenate)           (None, 144, 144, 96) 0           upA[0][0]                        \n",
      "                                                                 encodeAb[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "convOuta (Conv2D)               (None, 144, 144, 32) 27680       concatA[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "convOutb (Conv2D)               (None, 144, 144, 32) 9248        convOuta[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "PredictionMask (Conv2D)         (None, 144, 144, 1)  33          convOutb[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 7,846,945\n",
      "Trainable params: 7,846,945\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Writing model to './output/unet_model_for_decathlon.hdf5'\n"
     ]
    }
   ],
   "source": [
    "print(\"-\" * 30)\n",
    "print(\"Creating and compiling model ...\")\n",
    "print(\"-\" * 30)\n",
    "unet_model = unet()\n",
    "model = unet_model.create_model(imgs_train.shape, msks_train.shape)\n",
    "\n",
    "model_filename, model_callbacks = unet_model.get_callbacks()\n",
    "\n",
    "# # If there is a current saved file, then load weights and start from there.\n",
    "# saved_model = os.path.join(args.output_path, args.inference_filename)\n",
    "# if os.path.isfile(saved_model):\n",
    "#     model.load_weights(saved_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet below draws the model using Keras' built-in `plot_model`. Compare with the implementation of `model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\.conda\\envs\\decathlon\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;31m# to check the pydot/graphviz installation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'Dot'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-365228c5a6f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m                           \u001b[0mshow_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                           \u001b[0mshow_layer_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                           \u001b[0mrankdir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'TB'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m                          )\n\u001b[0;32m      7\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'images/model.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\decathlon\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[1;34m(model, to_file, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[0;32m    146\u001b[0m           \u001b[1;34m'LR'\u001b[0m \u001b[0mcreates\u001b[0m \u001b[0ma\u001b[0m \u001b[0mhorizontal\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m   \"\"\"\n\u001b[1;32m--> 148\u001b[1;33m   \u001b[0mdot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m   \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\decathlon\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[1;34m(model, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[0;32m     69\u001b[0m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m   \u001b[0m_check_pydot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m   \u001b[0mdot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m   \u001b[0mdot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rankdir'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\decathlon\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;31m# pydot raises a generic Exception here,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;31m# so no specific class can be caught.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m     raise ImportError('Failed to import pydot. You must install pydot'\n\u001b[0m\u001b[0;32m     50\u001b[0m                       ' and graphviz for `pydotprint` to work.')\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work."
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model,\n",
    "                          to_file='images/model.png',\n",
    "                          show_shapes=True,\n",
    "                          show_layer_names=True,\n",
    "                          rankdir='TB'\n",
    "                         )\n",
    "Image('images/model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Train the model on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Fitting model with training data ...\n",
      "------------------------------\n",
      "Step 3, training the model started at 2019-08-06 01:00:23.673330\n",
      "Train on 58464 samples, validate on 4608 samples\n",
      "Epoch 1/30\n",
      "  128/58464 [..............................] - ETA: 18:50:36 - loss: 2.9644 - acc: 0.1780 - dice_coef: 0.0397 - soft_dice_coef: 0.0372"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-41921214fca5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m               \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsks_validation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m               \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"batch\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m               callbacks=model_callbacks)\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Total time elapsed for training = {} seconds\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\decathlon\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\decathlon\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\decathlon\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\decathlon\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\decathlon\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1399\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1400\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"-\" * 30)\n",
    "print(\"Fitting model with training data ...\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"Step 3, training the model started at {}\".format(datetime.datetime.now()))\n",
    "start_time = time.time()\n",
    "\n",
    "if onnx:\n",
    "    for i in range(0,30):\n",
    "        print (\"Processing batch:\", i)\n",
    "        start_time = time.time()\n",
    "        model.train_on_batch(imgs_train[i*args.batch_size:args.batch_size*(i+1)-1], \\\n",
    "                             msks_train[i*args.batch_size:args.batch_size*(i+1)-1])\n",
    "        print (\"Time for training on batch:\", time.time() - start_time) \n",
    "\n",
    "        # TODO: Convert the Keras model to ONNX and save it. \n",
    "        onnx_model = onnxmltools.convert_keras(model, target_opset=7) \n",
    "        onnxmltools.utils.save_model(onnx_model, 'output/unet_model_for_decathlon.onnx')\n",
    "else:\n",
    "    history = model.fit(imgs_train, msks_train,\n",
    "              batch_size=args.batch_size,\n",
    "              epochs=args.epochs,\n",
    "              validation_data=(imgs_validation, msks_validation),\n",
    "              verbose=1, shuffle=\"batch\",\n",
    "              callbacks=model_callbacks)\n",
    "    \n",
    "print(\"Total time elapsed for training = {} seconds\".format(time.time() - start_time))\n",
    "print(\"Training finished at {}\".format(datetime.datetime.now()))\n",
    "    \n",
    "# Append training log\n",
    "# with open(\"training.log\",\"a+\") as fp:\n",
    "#     fp.write(\"{}: {}\\n\".format(datetime.datetime.now(),\n",
    "#                              history.history[\"val_dice_coef\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Evaluate the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\" * 30)\n",
    "print(\"Loading the best trained model ...\")\n",
    "print(\"-\" * 30)\n",
    "unet_model.evaluate_model(model_filename, imgs_testing, msks_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End: In this tutorial, you have learnt:\n",
    "* What is the U-Net model\n",
    "* Comparing training times - Tensorflow_MKLDNN vs Tensorflow (stock)\n",
    "* How to tweak a series of environment variables to get better performance out of MKLDNN\n",
    "* How to tweak a series of Tensorflow-related and neural-network specific parameters for better performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. SPDX-License-Identifier: EPL-2.0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Copyright (c) 2019 Intel Corporation`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
